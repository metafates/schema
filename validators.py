from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, TYPE_CHECKING, Protocol
from pathlib import Path
import tomllib

if TYPE_CHECKING:
    from _typeshed import SupportsWrite


@dataclass
class Import:
    path: str
    pkg: str


@dataclass
class ValidatorType:
    name: str
    constraint: str


@dataclass
class Validator:
    name: str
    internal: bool
    desc: str
    types: list[ValidatorType]
    embed: Optional[str]
    aliased: Optional[str]


@dataclass
class Data:
    validators: list[Validator]
    imports: set[str]


def read(path: str) -> Data:
    with open(path, "rb") as f:
        data = tomllib.load(f)

    imports_registry: dict[str, Import] = {}

    for import_ in data["imports"]:
        pkg = import_["pkg"]
        path = import_["path"]

        imports_registry[pkg] = Import(path=path, pkg=pkg)

    imports: set[str] = set()

    validators: list[Validator] = []

    for entry in data["validators"]:
        types: list[ValidatorType] = []

        for type_ in entry["types"]:
            validator_type = ValidatorType(
                name=type_["name"], constraint=type_["constraint"]
            )

            types.append(
                ValidatorType(name=type_["name"], constraint=type_["constraint"])
            )

            if "." in validator_type.constraint:
                pkg = validator_type.constraint.split(".")[0]

                if pkg not in imports_registry:
                    raise Exception(
                        f"unknown package for constraint: {validator_type.constraint}"
                    )

                imports.add(imports_registry[pkg].path)

        validator = Validator(
            name=entry["name"],
            internal=entry.get("internal", False),
            desc=entry["desc"],
            types=types,
            embed=entry.get("embed"),
            aliased=entry.get("aliased"),
        )

        validators.append(validator)

    return Data(validators=validators, imports=imports)


class P(Protocol):
    def __call__(self, *args: object) -> None: ...


def make_p(file: SupportsWrite[str]) -> P:
    def p(*s: object):
        print(*s, file=file)

    return p


def comment(s: str) -> str:
    commented_lines: list[str] = []

    for line in s.splitlines():
        commented_lines.append(f"// {line.rstrip()}".strip())

    return "\n".join(commented_lines)


PREAMBLE = "// Code generated by validators.py; DO NOT EDIT."


def generate_imports(file: SupportsWrite[str], imports: set[str]):
    if not len(imports):
        return

    p = make_p(file)
    p("import (")

    for path in sorted(imports):
        p(f'\t"{path}"')

    p(")")
    p()


def generate_validators(file: SupportsWrite[str], data: Data):
    p = make_p(file)

    p(PREAMBLE)
    p("package validate")
    p()

    generate_imports(file, data.imports)

    for v in data.validators:
        types_str = ""
        types = list(map(lambda t: f"{t.name} {t.constraint}", v.types))

        if len(types):
            types_str = f"[{', '.join(types)}]"

        embed = ""

        if v.embed:
            embed = f"\n\t{v.embed}\n"

        desc = comment(v.desc)

        if desc:
            p(desc)

        p(f"type {v.name}{types_str} struct{{{embed}}}")
        p()


def generate_aliases(file: SupportsWrite[str], data: Data, pkg: str):
    p = make_p(file)

    p(PREAMBLE)
    p(f"package {pkg}")
    p()

    imports = data.imports.copy()
    imports.add("github.com/metafates/schema/validate")

    generate_imports(file, imports)

    for v in filter(lambda v: not v.internal, data.validators):
        types_str = ""
        types = list(map(lambda t: f"{t.name} {t.constraint}", v.types))

        if len(types):
            types_str = f"[{', '.join(types)}]"

        desc = comment(v.desc)

        if desc:
            p(desc)

        aliased = v.aliased
        if not aliased:
            types = list(map(lambda t: t.name, v.types))
            aliased = f"Custom[{types[0]}, validate.{v.name}[{', '.join(types)}]]"

        p(f"type {v.name}{types_str} = {aliased}")
        p()


def generate_markdown(file: SupportsWrite[str], data: Data):
    p = make_p(file)

    p("# Validators")
    p("")
    p("This table features all available validators.")
    p("")
    p("| Name | Description |")
    p("| ---- | ----------- |")
    for v in data.validators:
        desc = "<br/>".join(v.desc.splitlines())
        types = list(map(lambda t: t.name, v.types))

        p(f"| `{v.name}[{', '.join(types)}]` | {desc} |")


def main():
    data = read("validators.toml")

    with Path("validate").joinpath("validators.go").open("w+") as out:
        generate_validators(out, data)

    with Path("required").joinpath("required.go").open("w+") as out:
        generate_aliases(out, data, pkg="required")

    with Path("optional").joinpath("optional.go").open("w+") as out:
        generate_aliases(out, data, pkg="optional")

    with Path("validators.md").open("w+") as out:
        generate_markdown(out, data)


if __name__ == "__main__":
    main()
